{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 17:06:10.067443: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-01 17:06:10.197484: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-01 17:06:10.225685: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-01 17:06:10.450872: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-01 17:06:12.160558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting line_profiler\n",
      "  Downloading line_profiler-4.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Downloading line_profiler-4.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (719 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.7/719.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: line_profiler\n",
      "Successfully installed line_profiler-4.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "from roaringbitmap import RoaringBitmap\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import csv\n",
    "import heapq as hq\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748808383.971451    2506 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1748808384.275878    2506 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1748808384.275974    2506 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1748808384.278907    2506 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1748808384.279104    2506 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1748808384.279194    2506 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1748808384.538188    2506 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1748808384.538298    2506 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-06-01 17:06:24.538318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1748808384.538411    2506 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-06-01 17:06:24.538968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4057 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "/home/igor/anaconda3/envs/tcc/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 42 variables whereas the saved optimizer has 46 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/home/igor/anaconda3/envs/tcc/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 30 variables whereas the saved optimizer has 34 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "tam = 30\n",
    "final_model_dnn_path = f\"modelos/dnn_model_{tam}.keras\"\n",
    "final_model_value_path = f\"modelos/dnn_value_model_{tam}.keras\"\n",
    "model_branching = load_model(final_model_dnn_path)\n",
    "model_bounding = load_model(final_model_value_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de branch and bound com paralelismo de bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smallest_last_degree(G):\n",
    "\n",
    "\tresidual_degree = {}\n",
    "\tQ = []\n",
    "\tfor v in G.nodes:\n",
    "\t\tresidual_degree[v] = len( G.adj[v] )\n",
    "\t\tQ.append( (residual_degree[v], v) )\n",
    "\n",
    "\thq.heapify(Q)\n",
    "\n",
    "\tS = []\n",
    "\twhile len(Q) > 0:\n",
    "\t\t(degree, v) = hq.heappop(Q)\n",
    "\t\tif v in S:\n",
    "\t\t\tcontinue\n",
    "\t\tS.append(v)\n",
    "\t\tfor u in G.adj[v]:\n",
    "\t\t\tif u not in S:\n",
    "\t\t\t\tresidual_degree[u] -= 1\n",
    "\t\t\t\thq.heappush(Q, (residual_degree[u], u))\n",
    "\tS.reverse()\n",
    "\treturn S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_kpartite(adj, V, l):\n",
    "    index = 1\n",
    "    R = V.copy()\n",
    "    sizeR = len(R)\n",
    "\n",
    "    while sizeR > 0 and index <= l:\n",
    "        S = R.copy()\n",
    "        while len(S) > 0:\n",
    "            v = S.min()\n",
    "            S -= adj[v]\n",
    "            R.remove(v)\n",
    "            S.remove(v)\n",
    "            sizeR -= 1\n",
    "        index += 1\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitcliquePartite(G, V, clique_atual, best_clique, number_of_nodes):\n",
    "    number_of_nodes[0] += 1\n",
    "    sizeV = len(V)\n",
    "\n",
    "    if sizeV == 0:\n",
    "        if len(clique_atual) > len(best_clique):\n",
    "            best_clique[:] = clique_atual[:]\n",
    "    else:\n",
    "        l = len(best_clique) - len(clique_atual)\n",
    "        R = max_kpartite(G, V, l)\n",
    "\n",
    "        order = list(R)\n",
    "\n",
    "        m = len(order)\n",
    "        for i in range(m-1, -1, -1):\n",
    "            v = order[i]\n",
    "            V2 = V.copy()\n",
    "            V2 &= G[v]\n",
    "            V.remove(v)\n",
    "            clique_atual.append(v)\n",
    "            bitcliquePartite(G, V2, clique_atual, best_clique, number_of_nodes)\n",
    "            clique_atual.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algoBitCliqueMaxPartite(G):\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tS = smallest_last_degree(G)\n",
    "\trotulo = {}\n",
    "\tfor i in range( len(S) ):\n",
    "\t\trotulo[ S[i] ] = i\n",
    "\tG = nx.relabel_nodes(G, rotulo)\n",
    "\n",
    "\tadj = []\n",
    "\tn = len(G.nodes)\n",
    "\tfor i in range( n ):\n",
    "\t\tadj.append(  RoaringBitmap( list( G.adj[i])  ) )\n",
    "\n",
    "\tclique_atual = []\n",
    "\tbest_clique = []\n",
    "\tV = RoaringBitmap( [ i for i in range(n) ])\n",
    "\tnumber_of_nodes = [ 0 ]\n",
    "\n",
    "\n",
    "\tbitcliquePartite( adj ,  V , clique_atual, best_clique, number_of_nodes )\n",
    "\n",
    "\tend_time = time.time()\n",
    "\telapsed_time = end_time - start_time\n",
    "\treturn best_clique, number_of_nodes, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmo de branch and bound com DLTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_state_representation(Q, K, G_dict_of_lists, tam):\n",
    "    vertex_list = list(K)\n",
    "    len_k = len(vertex_list)\n",
    "\n",
    "    vertex_to_idx = {vertex: i for i, vertex in enumerate(vertex_list)}\n",
    "\n",
    "    adj_matrix = np.zeros((tam, tam), dtype=np.float32)\n",
    "    clique_mask = np.zeros(tam, dtype=np.float32)\n",
    "\n",
    "    if not Q:\n",
    "        return adj_matrix, clique_mask\n",
    "\n",
    "    v_q = Q[-1]\n",
    "\n",
    "    G_adj_sets_for_K = {node_k: set(G_dict_of_lists.get(node_k, [])) for node_k in vertex_list}\n",
    "\n",
    "    neighbors_of_vq_in_G_set = set(G_dict_of_lists.get(v_q, []))\n",
    "    \n",
    "    neighbors_of_vq_in_K = neighbors_of_vq_in_G_set & K\n",
    "\n",
    "    if v_q in vertex_to_idx:\n",
    "        idx_vq = vertex_to_idx[v_q]\n",
    "        v_q_is_in_NvqK = (v_q in neighbors_of_vq_in_K)\n",
    "\n",
    "        for k_idx in range(len_k):\n",
    "            vk = vertex_list[k_idx]\n",
    "            vk_is_in_NvqK = (vk in neighbors_of_vq_in_K)\n",
    "\n",
    "            if v_q_is_in_NvqK or vk_is_in_NvqK:\n",
    "                if vk in neighbors_of_vq_in_G_set:\n",
    "                    adj_matrix[idx_vq, k_idx] = 1.0\n",
    "            \n",
    "            if vk_is_in_NvqK or v_q_is_in_NvqK:\n",
    "                if v_q in G_adj_sets_for_K[vk]:\n",
    "                    adj_matrix[k_idx, idx_vq] = 1.0\n",
    "\n",
    "    Q_set = set(Q)\n",
    "    for i in range(len_k):\n",
    "        if vertex_list[i] in Q_set:\n",
    "            clique_mask[i] = 1.0\n",
    "            \n",
    "    return adj_matrix, clique_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs_for_model(adj_matrix, clique_mask):\n",
    "    tam = adj_matrix.shape[0]\n",
    "\n",
    "    if tam == 0:\n",
    "        inputs = []\n",
    "    else:\n",
    "        inputs = np.vsplit(adj_matrix, tam)\n",
    "\n",
    "    inputs.append(clique_mask[np.newaxis, :])\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_bounding(Q, K, G, tam):\n",
    "    adj_matrix, clique_mask = build_state_representation(Q, K, G, tam)\n",
    "    inputs = build_inputs_for_model(adj_matrix, clique_mask)\n",
    "    pred = model_bounding.predict(inputs, verbose=0)\n",
    "    valor = pred[0, 0]  # extrai o escalar do array\n",
    "    return int(round(valor))\n",
    "\n",
    "def get_branching_dnn_predictions(Q, K, G_permuted, tam):\n",
    "    if not K: # Se K estiver vazio, não há o que pontuar.\n",
    "        return np.array([]) # Retorna um array vazio ou None, dependendo de como o chamador lida.\n",
    "\n",
    "    adj_matrix, clique_mask = build_state_representation(Q, K, G_permuted, tam)\n",
    "    inputs = build_inputs_for_model(adj_matrix, clique_mask)\n",
    "    pred_raw = model_branching.predict(inputs, verbose=0) # Saída é (1, tam)\n",
    "    return pred_raw[0]\n",
    "\n",
    "def should_expand(Q_new, K_new, C, G, tam):\n",
    "    est_total = len(Q_new) + dnn_bounding(Q_new, K_new, G, tam)\n",
    "    return est_total > len(C)\n",
    "\n",
    "def neighbors(G, v):\n",
    "    return set(G[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_instance(G_original):\n",
    "    original_node_labels = sorted(G_original.keys(), key=lambda v_orig: -len(G_original.get(v_orig, [])), reverse=False)\n",
    "\n",
    "    p_vertex_position = {v_orig: i_permuted for i_permuted, v_orig in enumerate(original_node_labels)}\n",
    "    p_vertex_at_position = {i_permuted: v_orig for i_permuted, v_orig in enumerate(original_node_labels)}\n",
    "\n",
    "    permuted_G = {}\n",
    "    num_nodes = len(original_node_labels)\n",
    "    for i_permuted in range(num_nodes):\n",
    "        v_orig = p_vertex_at_position[i_permuted]\n",
    "        original_neighbors = G_original.get(v_orig, [])\n",
    "        permuted_G[i_permuted] = {p_vertex_position[u_orig] for u_orig in original_neighbors}\n",
    "    \n",
    "    C_permuted = []\n",
    "    K_permuted_initial = set(permuted_G.keys()) # Nós de 0 a num_nodes-1\n",
    "    S = [([], K_permuted_initial)] # Q inicial vazio, K inicial com todos os nós permutados\n",
    "\n",
    "    return C_permuted, S, permuted_G, p_vertex_at_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_instance(C_permuted, p_vertex_at_position):\n",
    "    return [p_vertex_at_position[i_permuted] for i_permuted in C_permuted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCBB_DLTS(G_original, \n",
    "              tam,  \n",
    "              k_iteracoes_refresh_dnn_branching=2, \n",
    "              k_profundidade_para_dnn_bound=3):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    nodes_expandidos_count = 0\n",
    "\n",
    "    C_permuted, S, G_permuted, map_vertex_at_position = pre_process_instance(G_original)\n",
    "\n",
    "    scores_branching_atuais = None\n",
    "    iteracoes_desde_ultimo_refresh_branching = 0\n",
    "\n",
    "    while S:\n",
    "        Q_raiz_perm, K_raiz_perm = S.pop()\n",
    "        nodes_expandidos_count += 1\n",
    "\n",
    "        Q_caminho_perm = list(Q_raiz_perm)\n",
    "        K_caminho_perm = set(K_raiz_perm)\n",
    "\n",
    "        iteracoes_desde_ultimo_refresh_branching = 0\n",
    "        scores_branching_atuais = None \n",
    "\n",
    "        while K_caminho_perm:\n",
    "            extensao_estimada_clique = 0\n",
    "            \n",
    "            if k_profundidade_para_dnn_bound <= 0:\n",
    "                profundidade_check_dnn_bound = 0 \n",
    "            else:\n",
    "                profundidade_check_dnn_bound = len(Q_caminho_perm) % k_profundidade_para_dnn_bound\n",
    "            \n",
    "            if profundidade_check_dnn_bound == 0:\n",
    "                extensao_estimada_clique = dnn_bounding(\n",
    "                                                Q_caminho_perm, \n",
    "                                                K_caminho_perm, \n",
    "                                                G_permuted, tam)\n",
    "            else:\n",
    "                extensao_estimada_clique = len(K_caminho_perm) \n",
    "\n",
    "            if not (len(C_permuted) < len(Q_caminho_perm) + extensao_estimada_clique):\n",
    "                break\n",
    "\n",
    "            if iteracoes_desde_ultimo_refresh_branching % k_iteracoes_refresh_dnn_branching == 0 or \\\n",
    "                scores_branching_atuais is None:\n",
    "                scores_branching_atuais = get_branching_dnn_predictions(\n",
    "                                                Q_caminho_perm, K_caminho_perm, \n",
    "                                                G_permuted, tam)\n",
    "            \n",
    "            iteracoes_desde_ultimo_refresh_branching += 1\n",
    "            \n",
    "            v_perm = None\n",
    "            if scores_branching_atuais is not None and len(scores_branching_atuais) == tam:\n",
    "                scores_candidatos_K = {idx_node: scores_branching_atuais[idx_node] \n",
    "                                    for idx_node in K_caminho_perm}\n",
    "                if scores_candidatos_K:\n",
    "                    v_perm = max(scores_candidatos_K, key=scores_candidatos_K.get)\n",
    "            \n",
    "            if v_perm is None:\n",
    "                break \n",
    "            \n",
    "            K_caminho_perm_sem_v = set(K_caminho_perm)\n",
    "            K_caminho_perm_sem_v.remove(v_perm)\n",
    "\n",
    "            if K_caminho_perm_sem_v:\n",
    "                if should_expand(Q_caminho_perm, K_caminho_perm_sem_v, C_permuted, G_permuted, tam):\n",
    "                    S.append((list(Q_caminho_perm), K_caminho_perm_sem_v))\n",
    "            \n",
    "            Q_caminho_perm.append(v_perm)\n",
    "            K_caminho_perm = K_caminho_perm_sem_v.intersection(neighbors(G_permuted, v_perm))\n",
    "        \n",
    "        if len(C_permuted) < len(Q_caminho_perm):\n",
    "            C_permuted = list(Q_caminho_perm)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    C_final_original = post_process_instance(C_permuted, map_vertex_at_position)\n",
    "    \n",
    "    return C_final_original, nodes_expandidos_count, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparação entre os Algoritmos de branch and bound com paralelismo de bits e com DLTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ \"nodes\", \"densidade\", \"DLTS Clique (tempo)\" , \"algoBitCliqueMaxPartite (tempo)\", \"DLTS Clique (nodes)\", \"algoBitCliqueMaxPartite (nodes)\", \"Dif\"]\n",
    "with open('exato.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "  writer = csv.writer(f)\n",
    "  writer.writerow(data)\n",
    "\n",
    "valores1 = [10, 15, 20, 30]\n",
    "valores2 = [0.25, 0.5, 0.75, 0.9]\n",
    "\n",
    "combinacoes = list(product(valores1, valores2))\n",
    "for c in combinacoes:\n",
    "  for i in range(10):\n",
    "    nodes = c[0]\n",
    "    densidade = c[1]\n",
    "    G = nx.gnp_random_graph(nodes, densidade)\n",
    "    G_dict = nx.to_dict_of_lists(G)\n",
    "    best_clique1, number_of_nodes1, elapsed_time1 = MCBB_DLTS(G_dict, tam)\n",
    "    best_clique2, number_of_nodes2, elapsed_time2 = algoBitCliqueMaxPartite(G)\n",
    "    # comparar se o tamanho do clique é o mesmo\n",
    "    tam_dlts = len(best_clique1)\n",
    "    tam_exato = len(best_clique2)\n",
    "    dif = (tam_exato - tam_dlts) / tam_exato\n",
    "\n",
    "    data = [ nodes, densidade, elapsed_time1, elapsed_time2, number_of_nodes1, number_of_nodes2[0], dif ]\n",
    "    with open('exato.csv', 'a', encoding='UTF8', newline='') as f:\n",
    "      writer = csv.writer(f)\n",
    "      writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_agrupados = []\n",
    "try:\n",
    "    with open('exato.csv', 'r', encoding='UTF8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header_exato = next(reader) # Pula o cabeçalho do exato.csv\n",
    "\n",
    "        current_group = []\n",
    "        i = 0\n",
    "        for row_idx, row in enumerate(reader):\n",
    "            converted_row = []\n",
    "            # Colunas esperadas no exato.csv e seus índices:\n",
    "            # 0: nodes (ignorado aqui, pois será pego de 'combinacoes' depois)\n",
    "            # 1: densidade (ignorado aqui)\n",
    "            # 2: DLTS Clique (tempo) -> float\n",
    "            # 3: algoBitCliqueMaxPartite (tempo) -> float\n",
    "            # 4: DLTS Clique (nodes) -> int\n",
    "            # 5: algoBitCliqueMaxPartite (nodes) -> int\n",
    "            # 6: Dif -> float\n",
    "            try:\n",
    "                # Adiciona apenas as colunas de dados numéricos que serão promediadas\n",
    "                converted_row.append(float(row[2])) # DLTS Tempo\n",
    "                converted_row.append(float(row[3])) # BitClique Tempo\n",
    "                converted_row.append(int(row[4]))   # DLTS Nodes\n",
    "                converted_row.append(int(row[5]))   # BitClique Nodes\n",
    "                converted_row.append(float(row[6])) # Dif\n",
    "            except ValueError as e:\n",
    "                print(f\"Erro ao converter valor na linha {row_idx + 2} do exato.csv: {e}. Linha: {row}\")\n",
    "                continue # Pula esta linha se houver erro de conversão\n",
    "\n",
    "            current_group.append(converted_row)\n",
    "            i += 1\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                if current_group: \n",
    "                    dados_agrupados.append(current_group)\n",
    "                current_group = []\n",
    "        \n",
    "        if current_group: # Adiciona o último grupo se ele não completou 10 linhas\n",
    "            dados_agrupados.append(current_group)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: O arquivo 'exato.csv' não foi encontrado.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao ler 'exato.csv': {e}\")\n",
    "    exit()\n",
    "\n",
    "# Parte 2: Cálculo das Médias\n",
    "medias_agrupadas = []\n",
    "if not dados_agrupados:\n",
    "    print(\"Nenhum dado foi agrupado de 'exato.csv'. Verifique o arquivo e a lógica de agrupamento.\")\n",
    "else:\n",
    "    for grupo_idx, grupo in enumerate(dados_agrupados):\n",
    "        if not grupo:\n",
    "            print(f\"Aviso: Grupo {grupo_idx} está vazio. Pulando.\")\n",
    "            continue\n",
    "\n",
    "        media_grupo = []\n",
    "        num_colunas_para_media = 5 \n",
    "        for col_idx in range(num_colunas_para_media):\n",
    "            coluna_valores = []\n",
    "            for linha in grupo:\n",
    "                if col_idx < len(linha):\n",
    "                    coluna_valores.append(linha[col_idx])\n",
    "                else:\n",
    "                    print(f\"Aviso: Linha no grupo {grupo_idx} não tem coluna {col_idx}. Linha: {linha}\")\n",
    "\n",
    "            if coluna_valores:\n",
    "                media_calculada = sum(coluna_valores) / len(coluna_valores)\n",
    "                media_grupo.append(media_calculada)\n",
    "            else:\n",
    "                print(f\"Aviso: Coluna {col_idx} no grupo {grupo_idx} não tem valores. Usando 0.0 para média.\")\n",
    "                media_grupo.append(0.0) \n",
    "        medias_agrupadas.append(media_grupo)\n",
    "\n",
    "# Parte 3: Escrita de 'medias.csv' com formatação\n",
    "if 'combinacoes' not in locals() or not isinstance(combinacoes, list) or not combinacoes:\n",
    "    print(\"Erro: A variável 'combinacoes' não está definida, não é uma lista ou está vazia.\")\n",
    "    print(\"Defina 'combinacoes' como uma lista de tuplas (nodes, densidade) antes de executar esta parte.\")\n",
    "    # Exemplo para demonstração (REMOVA OU SUBSTITUA PELA SUA LÓGICA REAL)\n",
    "    # Se você tem 16 grupos em medias_agrupadas, combinacoes deve ter 16 pares.\n",
    "    # Ex: combinacoes = [(10,0.25)]*len(medias_agrupadas) # Placeholder\n",
    "    exit()\n",
    "\n",
    "if len(combinacoes) != len(medias_agrupadas):\n",
    "    print(f\"Erro: O número de combinações ({len(combinacoes)}) não corresponde ao número de grupos de médias ({len(medias_agrupadas)}).\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    with open('medias.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        header_medias = [\"nodes\", \"densidade\", \"DLTS Clique (tempo medio)\", \n",
    "                         \"algoBitCliqueMaxPartite (tempo medio)\", \"DLTS Clique (nodes medio)\", \n",
    "                         \"algoBitCliqueMaxPartite (nodes medio)\", \"Dif medio\"]\n",
    "        writer.writerow(header_medias)\n",
    "        \n",
    "        for i, (nodes_val, densidade_val) in enumerate(combinacoes):\n",
    "            if i < len(medias_agrupadas) and len(medias_agrupadas[i]) == 5:\n",
    "                data_row = [\n",
    "                    nodes_val,\n",
    "                    densidade_val,\n",
    "                    \"{:.2e}\".format(medias_agrupadas[i][0]),  # DLTS Clique (tempo medio)\n",
    "                    \"{:.2e}\".format(medias_agrupadas[i][1]),  # algoBitCliqueMaxPartite (tempo medio)\n",
    "                    \"{:.2e}\".format(medias_agrupadas[i][2]),  # DLTS Clique (nodes medio)\n",
    "                    \"{:.2e}\".format(medias_agrupadas[i][3]),  # algoBitCliqueMaxPartite (nodes medio)\n",
    "                    \"{:.2f}\".format(medias_agrupadas[i][4])   # Dif medio - FORMATADO COMO X.XX\n",
    "                ]\n",
    "                writer.writerow(data_row)\n",
    "            else:\n",
    "                print(f\"Aviso: Dados de média ausentes ou incompletos para combinação {i} ({nodes_val}, {densidade_val}). Pulando escrita.\")\n",
    "\n",
    "    print(\"Arquivo medias.csv salvo com sucesso e com a formatação desejada!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro ao escrever 'medias.csv': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "csv_data_medias = \"\"\"nodes,densidade,DLTS Clique (tempo medio),algoBitCliqueMaxPartite (tempo medio),DLTS Clique (nodes medio),algoBitCliqueMaxPartite (nodes medio),Dif medio\n",
    "10,0.25,7.18,0.0,13.5,1.38e+01,0.00e+00\n",
    "10,0.5,7.77,0.0,17.0,1.71e+01,0.00e+00\n",
    "10,0.75,5.83,0.0,13.2,2.53e+01,2.00e-02\n",
    "10,0.9,5.25,0.0,11.3,3.58e+01,3.00e-02\n",
    "15,0.25,10.99,0.0,25.7,1.96e+01,0.00e+00\n",
    "15,0.5,9.05,0.0,20.9,2.95e+01,3.00e-02\n",
    "15,0.75,10.16,0.0,19.6,4.40e+01,3.00e-02\n",
    "15,0.9,7.38,0.0,12.8,6.68e+01,3.00e-02\n",
    "20,0.25,17.04,0.0,39.3,2.61e+01,0.00e+00\n",
    "20,0.5,16.8,0.0,36.0,3.47e+01,0.00e+00\n",
    "20,0.75,9.06,0.0,16.2,6.50e+01,5.00e-02\n",
    "20,0.9,9.83,0.0,16.1,9.67e+01,6.00e-02\n",
    "30,0.25,39.22,0.0,94.0,4.00e+01,0.00e+00\n",
    "30,0.5,26.17,0.0,50.4,5.74e+01,8.00e-02\n",
    "30,0.75,12.09,0.0,21.2,1.04e+02,1.00e-01\n",
    "30,0.9,16.13,0.0,25.5,1.74e+02,4.00e-02\n",
    "\"\"\"\n",
    "\n",
    "# Usa io.StringIO para tratar a string como um arquivo\n",
    "data_file_like_object = io.StringIO(csv_data_medias)\n",
    "\n",
    "# Lê os dados CSV para um DataFrame do pandas\n",
    "df_medias = pd.read_csv(data_file_like_object)\n",
    "\n",
    "# Imprime o DataFrame. Ele será formatado como uma tabela.\n",
    "print(df_medias.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
