{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import (f1_score,\n",
    "                            roc_auc_score,\n",
    "                            roc_curve,\n",
    "                            average_precision_score,\n",
    "                            precision_recall_curve,\n",
    "                            mean_absolute_error,\n",
    "                            mean_squared_error,\n",
    "                            root_mean_squared_error,\n",
    "                            r2_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_dir = \"train_graphs\" #caminho dos dados para treinar o modelo\n",
    "tam = 30\n",
    "final_model_dnn_path = f\"modelos/dnn_model_{tam}.keras\" #caminho do modelo final\n",
    "final_model_value_path = f\"modelos/dnn_value_model_{tam}.keras\" #caminho dos valores finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-Score\n",
    "def calcular_f1(true_values, predictions, average='binary'):\n",
    "    return f1_score(true_values, predictions, average=average)\n",
    "\n",
    "# AUC-ROC\n",
    "def auc_roc(true_values, predictions):\n",
    "    fpr, tpr, _ = roc_curve(true_values, predictions)\n",
    "    roc = roc_auc_score(true_values, predictions)\n",
    "    return fpr, tpr, roc\n",
    "\n",
    "# AUC-PR\n",
    "def auc_pr(true_values, predictions):\n",
    "    precision, recall, _ = precision_recall_curve(true_values, predictions)\n",
    "    pr = average_precision_score(true_values, predictions)\n",
    "    return precision, recall, pr\n",
    "\n",
    "# IoU\n",
    "def iou_score(true_values, predictions):\n",
    "    intersection = np.sum(np.logical_and(true_values, predictions))  # Elementos em comum (1s)\n",
    "    union = np.sum(np.logical_or(true_values, predictions))          # União de elementos\n",
    "    \n",
    "    plt.figure(figsize=(6, 2))\n",
    "    plt.bar(range(len(true_values)), true_values, color='green', alpha=0.6, label='Real')\n",
    "    plt.bar(range(len(predictions)), predictions, color='red', alpha=0.4, label='Predito')\n",
    "    plt.bar(range(len(intersection)), intersection, color='blue', alpha=0.7, label='Interseção')\n",
    "\n",
    "    plt.xlabel('Vértices')\n",
    "    plt.ylabel('Presença na Clique')\n",
    "    plt.title('Visualização da IoU')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return intersection / union if union != 0 else 0.0\n",
    "\n",
    "# MAE\n",
    "def mae(true_values, predictions):\n",
    "    return mean_absolute_error(true_values, predictions)\n",
    "\n",
    "# MSE\n",
    "def mse(true_values, predictions):\n",
    "    return mean_squared_error(true_values, predictions)\n",
    "\n",
    "# RMSE\n",
    "def rmse(true_values, predictions):\n",
    "    return root_mean_squared_error(true_values, predictions)\n",
    "\n",
    "# R2\n",
    "def r2(true_values, predictions):\n",
    "    return r2_score(true_values, predictions)\n",
    "\n",
    "def plot_predictions(y_true, y_pred):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, alpha=0.7)\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], color='red', linestyle='--')\n",
    "    plt.xlabel(\"Valores Reais (y_test)\")\n",
    "    plt.ylabel(\"Previsões (y_pred)\")\n",
    "    plt.title(\"Comparação entre Valores Reais e Previstos\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando todos os arquivos do diretório\n",
    "files = sorted([os.path.basename(ii) for ii in glob.glob(f\"{labeled_data_dir}/*.dimacs\")])\n",
    "_, files_teste = train_test_split(files, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_pointer(fp):\n",
    "    lines = [ll.strip() for ll in fp]\n",
    "    ii = 0\n",
    "    labels = []\n",
    "    res = []\n",
    "    cli = []\n",
    "    numLinhas = 0\n",
    "    while ii < len(lines):\n",
    "        line = lines[ii]\n",
    "        #contando o numero de vertices do grafo\n",
    "        if \"cliqueatual\" not in line:\n",
    "            ii += 1\n",
    "            numLinhas += 1\n",
    "            continue\n",
    "\n",
    "        #pegando a clique atual\n",
    "        if ii+1 >= len(lines):\n",
    "            break\n",
    "        line = line[3:]\n",
    "        spritado = line.split()\n",
    "        clique = [int(elem) for elem in spritado[1:]]\n",
    "        if(numLinhas < tam):\n",
    "            dif = tam - numLinhas\n",
    "            clique.extend([0]*dif)\n",
    "        cli.append(clique)\n",
    "\n",
    "        #criando o vetor de movimento\n",
    "        line = lines[ii+1]\n",
    "        sp = line.split()\n",
    "        mv = int(sp[-1])\n",
    "        label = [0] * tam\n",
    "        label[mv-1] = 1\n",
    "        labels.append(label)\n",
    "\n",
    "        #lendo o grafo\n",
    "        cells = []\n",
    "        for tt in range(numLinhas, 0, -1):\n",
    "            cell_line = lines[ii - tt][3:]\n",
    "            cells.extend([int(float(cc)) for cc in cell_line.split(\", \")])\n",
    "            if(numLinhas < tam):\n",
    "                dif = tam - numLinhas\n",
    "                cells.extend([0]*dif)\n",
    "        while len(cells) < tam * tam:\n",
    "            cells.extend([0]*tam)\n",
    "        res.append(cells)\n",
    "        ii += (numLinhas+2)\n",
    "    labels_v = list(range(len(labels),0, -1))\n",
    "    return (res, cli, labels, labels_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estruturar_entrada(batch_input, batch_labels):\n",
    "    # Dividir a entrada em uma lista de 151 tensores de forma (batch_size, tam)\n",
    "    batch_input_list = [batch_input[:, i, :] for i in range(batch_input.shape[1])]\n",
    "            \n",
    "    # Converter para tensores do TensorFlow\n",
    "    x_batch = [tf.convert_to_tensor(tensor, dtype=tf.float32) for tensor in batch_input_list]\n",
    "    input_dict = {f'input_{i}': tensor for i, tensor in enumerate(x_batch)}\n",
    "    y_batch = np.array(batch_labels)\n",
    "    return input_dict, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinar_entrada(res, clique, labels, remaining_batch_input=[], remaining_batch_labels=[]):\n",
    "    combined_input = np.array([np.hsplit(np.concatenate([res[i], clique[i]]), tam + 1) for i in range(len(clique))])\n",
    "    if len(remaining_batch_input) != 0:\n",
    "        combined_input = np.concatenate((remaining_batch_input, combined_input), axis=0)\n",
    "        labels = remaining_batch_labels + labels\n",
    "    return combined_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dir(files):\n",
    "    res = []\n",
    "    cli = []\n",
    "    labels = []\n",
    "    labels_v = []\n",
    "    random.seed(42)\n",
    "    random.shuffle(files)\n",
    "    random.seed()\n",
    "    for ff in files:\n",
    "        with open(os.path.join(labeled_data_dir,ff), 'r') as fp:\n",
    "            rr, cc, ll, ll_v = parse_file_pointer(fp)\n",
    "            res.extend(rr)\n",
    "            cli.extend(cc)\n",
    "            labels.extend(ll)\n",
    "            labels_v.extend(ll_v)\n",
    "    return res, cli, labels, labels_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_arquivos(batch_files, value):\n",
    "    res, clique, labels, labels_v = parse_dir(batch_files)\n",
    "    if value:\n",
    "        labels = labels_v\n",
    "    return res, clique, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para testar o modelo\n",
    "def testar_modelo(modelo, files_test, usar_labels_value):\n",
    "    res, clique, labels = ler_arquivos(files_test, usar_labels_value)\n",
    "    combined_input, labels = combinar_entrada(res, clique, labels)\n",
    "    x_test, y_test = estruturar_entrada(combined_input, labels)\n",
    "    # Prever usando o modelo\n",
    "    previsoes = modelo.predict(x_test)\n",
    "    \n",
    "    return y_test, previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Carregando o modelo branch...\")\n",
    "modelo_branch = load_model(final_model_dnn_path)\n",
    "print(\"Testando o modelo branch...\")\n",
    "y_test, previsoes = testar_modelo(modelo_branch, files_teste, False)\n",
    "\n",
    "# F1\n",
    "f1 = f1_score(y_test, previsoes)\n",
    "print(f'F1-score: {f1:.4f}')\n",
    "\n",
    "# AUC ROC\n",
    "fpr, tpr, auc_roc = auc_roc(y_test, previsoes)\n",
    "# Plotando a curva ROC\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUC-ROC = {auc_roc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Linha diagonal (azar)\n",
    "plt.xlabel('Taxa de Falsos Positivos')\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'AUC-ROC: {auc_roc:.4f}')\n",
    "\n",
    "# AUC PR\n",
    "precision, recall, pr = auc_pr(y_test, previsoes)\n",
    "# Plotando a curva Precisão-Recall\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(recall, precision, label=f'AUC-PR = {pr:.2f}', color='blue')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precisão')\n",
    "plt.title('Curva de Precisão-Recall')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'AUC-PR: {pr:.4f}')\n",
    "\n",
    "# IoU\n",
    "iou = iou_score(np.array(y_test), np.array(previsoes))\n",
    "print(f'IoU: {iou:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Carregando o modelo bound...\")\n",
    "modelo_bound = load_model(final_model_value_path)\n",
    "print(\"Testando o modelo bound...\")\n",
    "y_test, previsoes = testar_modelo(modelo_bound, files_teste, True)\n",
    "\n",
    "# MAE\n",
    "mae = mae(y_test, previsoes)\n",
    "print(f'MAE: {mae:.4f}')\n",
    "\n",
    "# MSE\n",
    "mse = mse(y_test, previsoes)\n",
    "print(f'MSE: {mse:.4f}')\n",
    "\n",
    "# RMSE\n",
    "rmse = rmse(y_test, previsoes)\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "\n",
    "# R2\n",
    "r2 = r2(y_test, previsoes)\n",
    "print(f'R²: {r2:.4f}')\n",
    "\n",
    "plot_predictions(y_test, previsoes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
