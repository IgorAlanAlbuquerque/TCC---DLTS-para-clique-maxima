{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8cd5c9-fee6-4564-8736-438c67979782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras import callbacks, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpa a sessão antes de construir o modelo\n",
    "clear_session()\n",
    "\n",
    "# Configurações de GPU\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "# Define a política de precisão mista\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "# Libera memória do CUDA\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "cuda.select_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d8688",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"modelos\" #caminha onde é salvo o modelo\n",
    "files_per_batch = 1000\n",
    "labeled_data_dir = \"train_graphs\" #caminho dos dados para treinar o modelo\n",
    "param_v_a_1 = [4, 3, 2] #camadas compartilhadas rede bound\n",
    "param_v_a_2 = [3, 2, 2] #camadas ocultas\n",
    "param_p_a_1 = [6, 4, 3] #camadas compartilhadas rede brach\n",
    "param_p_a_2 = [9, 6, 2] #camadas ocultas\n",
    "param_p_b = 128 #batch size\n",
    "param_v_b =  128\n",
    "param_p_l = 1e-4 #taxa de aprendizado\n",
    "param_v_l = 1e-4\n",
    "tam = 30\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando todos os arquivos do diretório\n",
    "files = sorted([os.path.basename(ii) for ii in glob.glob(f\"{labeled_data_dir}/*.dimacs\")])\n",
    "files_treino, _ = train_test_split(files, test_size=0.1, random_state=42)\n",
    "files_t, files_v = train_test_split(files_treino, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128523a-a737-4d05-8a04-4d461db04a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_pointer(fp):\n",
    "    lines = [ll.strip() for ll in fp]\n",
    "    ii = 0\n",
    "    labels = []\n",
    "    res = []\n",
    "    cli = []\n",
    "    numLinhas = 0\n",
    "    while ii < len(lines):\n",
    "        line = lines[ii]\n",
    "        #contando o numero de vertices do grafo\n",
    "        if \"cliqueatual\" not in line:\n",
    "            ii += 1\n",
    "            numLinhas += 1\n",
    "            continue\n",
    "\n",
    "        #pegando a clique atual\n",
    "        if ii+1 >= len(lines):\n",
    "            break\n",
    "        line = line[3:]\n",
    "        spritado = line.split()\n",
    "        clique = [int(elem) for elem in spritado[1:]]\n",
    "        if(numLinhas < tam):\n",
    "            dif = tam - numLinhas\n",
    "            clique.extend([0]*dif)\n",
    "        cli.append(clique)\n",
    "\n",
    "        #criando o vetor de movimento\n",
    "        line = lines[ii+1]\n",
    "        sp = line.split()\n",
    "        mv = int(sp[-1])\n",
    "        label = [0] * tam\n",
    "        label[mv-1] = 1\n",
    "        labels.append(label)\n",
    "\n",
    "        #lendo o grafo\n",
    "        cells = []\n",
    "        for tt in range(numLinhas, 0, -1):\n",
    "            cell_line = lines[ii - tt][3:]\n",
    "            cells.extend([int(float(cc)) for cc in cell_line.split(\", \")])\n",
    "            if(numLinhas < tam):\n",
    "                dif = tam - numLinhas\n",
    "                cells.extend([0]*dif)\n",
    "        while len(cells) < tam * tam:\n",
    "            cells.extend([0]*tam)\n",
    "        res.append(cells)\n",
    "        ii += (numLinhas+2)\n",
    "    labels_v = list(range(len(labels),0, -1))\n",
    "    return (res, cli, labels, labels_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82174a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estruturar_entrada(batch_input, batch_labels):\n",
    "    # Dividir a entrada em uma lista de 151 tensores de forma (batch_size, tam)\n",
    "    batch_input_list = [batch_input[:, i, :] for i in range(batch_input.shape[1])]\n",
    "            \n",
    "    # Converter para tensores do TensorFlow\n",
    "    x_batch = [tf.convert_to_tensor(tensor, dtype=tf.float32) for tensor in batch_input_list]\n",
    "    input_dict = {f'input_{i}': tensor for i, tensor in enumerate(x_batch)}\n",
    "    y_batch = np.array(batch_labels)\n",
    "    return input_dict, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ee89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinar_entrada(res, clique, labels, remaining_batch_input=[], remaining_batch_labels=[]):\n",
    "    combined_input = np.array([np.hsplit(np.concatenate([res[i], clique[i]]), tam + 1) for i in range(len(clique))])\n",
    "    if len(remaining_batch_input) != 0:\n",
    "        combined_input = np.concatenate((remaining_batch_input, combined_input), axis=0)\n",
    "        labels = remaining_batch_labels + labels\n",
    "    return combined_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6a7c5-ea3e-47a2-a1d3-d168e03916d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dir(files):\n",
    "    res = []\n",
    "    cli = []\n",
    "    labels = []\n",
    "    labels_v = []\n",
    "    random.seed(42)\n",
    "    random.shuffle(files)\n",
    "    random.seed()\n",
    "    for ff in files:\n",
    "        with open(os.path.join(labeled_data_dir,ff), 'r') as fp:\n",
    "            rr, cc, ll, ll_v = parse_file_pointer(fp)\n",
    "            res.extend(rr)\n",
    "            cli.extend(cc)\n",
    "            labels.extend(ll)\n",
    "            labels_v.extend(ll_v)\n",
    "    return res, cli, labels, labels_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa8fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_arquivos(batch_files, value):\n",
    "    res, clique, labels, labels_v = parse_dir(batch_files)\n",
    "    if value:\n",
    "        labels = labels_v\n",
    "    return res, clique, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size, value, is_train):\n",
    "    # Gerador de treinamento\n",
    "    def create_generator(file_list):\n",
    "        def generator():\n",
    "            idx = 0\n",
    "            remaining_batch_input = remaining_batch_labels = []\n",
    "            while idx < len(file_list):\n",
    "                batch_files = file_list[idx: idx + files_per_batch]\n",
    "                res, clique, labels = ler_arquivos(batch_files, value)\n",
    "                combined_input, labels = combinar_entrada(res, clique, labels, remaining_batch_input, remaining_batch_labels)\n",
    "                remaining_batch_input = remaining_batch_labels = []\n",
    "                num_samples = len(combined_input)\n",
    "                num_batches = num_samples // batch_size\n",
    "                for batch_idx in range(num_batches):\n",
    "                    batch_input = combined_input[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
    "                    batch_labels = labels[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
    "                    x_batch, y_batch = estruturar_entrada(batch_input, batch_labels)\n",
    "                    yield x_batch, y_batch\n",
    "                idx += files_per_batch\n",
    "                remaining_samples = num_samples % batch_size\n",
    "                if remaining_samples > 0:\n",
    "                    remaining_batch_input = combined_input[-remaining_samples:]\n",
    "                    remaining_batch_labels = labels[-remaining_samples:]\n",
    "        return generator\n",
    "\n",
    "    return create_generator(files_t)() if is_train else create_generator(files_v)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_signature_type_branch(batch_size):\n",
    "        output_signature=(\n",
    "                {f\"input_{i}\": tf.TensorSpec(shape=(batch_size, tam), dtype=tf.float32) for i in range(tam + 1)},  # Entradas\n",
    "                tf.TensorSpec(shape=(batch_size, tam), dtype=tf.float32)  # Rótulos\n",
    "        )\n",
    "        return output_signature\n",
    "\n",
    "def output_signature_type_bound(batch_size):\n",
    "        output_signature=(\n",
    "                {f\"input_{i}\": tf.TensorSpec(shape=(batch_size, tam), dtype=tf.float32) for i in range(tam + 1)},  # Entradas\n",
    "                tf.TensorSpec(shape=(batch_size,), dtype=tf.float32)  # Rótulos\n",
    "        )\n",
    "        return output_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae619989-ab64-4168-9a12-bc575a916901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class printbatch(callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        logging.info(\"Epoch: \"+ str(epoch))\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logging.info(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809c21c-f5cb-4135-9ae5-e19e265a00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#treinar rede branch\n",
    "#verificar se já tem um modelo compilado para treinar\n",
    "initial_epoch = 0\n",
    "checkpoint_dir = os.path.join(output_path, \"checkpoints\", f\"dnn_model_{tam}\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint_epoch_{epoch:04d}.keras\")\n",
    "\n",
    "if latest_checkpoint:\n",
    "    print(f\"Carregando modelo do checkpoint: {latest_checkpoint}\")\n",
    "    model = tf.keras.models.load_model(latest_checkpoint)\n",
    "    # Extraindo a última época treinada\n",
    "    initial_epoch = int(latest_checkpoint.split(\"_epoch_\")[1].split(\".keras\")[0])\n",
    "\n",
    "else:\n",
    "    # Definir camadas de entrada\n",
    "    inputArray = [Input(shape=(tam,), name=f\"input_{i}\") for i in range(tam + 1)]\n",
    "\n",
    "    # Camadas densas compartilhadas\n",
    "    layer = inputArray\n",
    "    for i in range(len(param_p_a_1)):\n",
    "        shared_dense = Dense(tam * param_p_a_1[i], activation='relu')\n",
    "        layer = [Dropout(0.3)(shared_dense(l)) for l in layer]\n",
    "\n",
    "    # Concatenar os vetores processados\n",
    "    merged_vector = Concatenate(axis=-1)(layer)\n",
    "\n",
    "    #camadas internas\n",
    "    layer = merged_vector\n",
    "    for i in range(len(param_p_a_2)):\n",
    "        layer = Dense(tam*(tam+1)*param_p_a_2[i], activation='relu', kernel_regularizer=l2(1e-3))(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = Dropout(0.3)(layer)\n",
    "\n",
    "    #camada de saida\n",
    "    output_layer = Dense(tam, activation='softmax')(layer)\n",
    "\n",
    "    #compilar modelo\n",
    "    model = Model(inputs=inputArray, outputs=output_layer)\n",
    "    adam = optimizers.Adam(learning_rate=param_p_l, clipnorm=1.0)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Criar os datasets de treino\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator,  # Gera um novo generator a cada época\n",
    "    args=(param_p_b, False, True),\n",
    "    output_signature=output_signature_type_branch(param_p_b)\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#dataset de validação\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator,\n",
    "    args=(param_p_b, False, False),\n",
    "    output_signature=output_signature_type_branch(param_p_b)\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "now = datetime.now()\n",
    "# Treinar modelo\n",
    "print(\"treinamento rede branch iniciado\")\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1000,\n",
    "    initial_epoch=initial_epoch,\n",
    "    verbose=1,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[\n",
    "        printbatch(),\n",
    "        EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True),\n",
    "        ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=False),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "    ]\n",
    ")\n",
    "final_model_dnn_path = os.path.join(output_path, f\"dnn_model_{tam}.keras\")\n",
    "model.save(final_model_dnn_path)\n",
    "del model, inputArray, merged_vector, layer, output_layer, train_dataset, val_dataset\n",
    "print(\"rede 1 treinada\")\n",
    "clear_session()\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "cuda.select_device(0)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b339dcc-d277-407b-8efc-e54f05d09306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#treinar rede bound\n",
    "checkpoint_dir = os.path.join(output_path, \"checkpoints\", f\"dnn_value_model_{tam}\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint_epoch_{epoch:04d}.keras\")\n",
    "\n",
    "# Verificar se existe um checkpoint salvo\n",
    "initial_epoch = 0\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if latest_checkpoint:\n",
    "    print(f\"Carregando modelo do checkpoint: {latest_checkpoint}\")\n",
    "    model = tf.keras.models.load_model(latest_checkpoint)\n",
    "    # Extraindo a última época treinada\n",
    "    initial_epoch = int(latest_checkpoint.split(\"_epoch_\")[1].split(\".keras\")[0])\n",
    "\n",
    "else:\n",
    "    # Definir camadas de entrada\n",
    "    inputArray = [Input(shape=(tam,), name=f\"input_{i}\") for i in range(tam + 1)]\n",
    "\n",
    "    # Camadas densas compartilhadas\n",
    "    layer = inputArray\n",
    "    for i in range(len(param_v_a_1)):\n",
    "        shared_dense = Dense(tam * param_v_a_1[i], activation='relu')\n",
    "        layer = [Dropout(0.1)(shared_dense(l)) for l in layer]\n",
    "\n",
    "    # Concatenar os vetores processados\n",
    "    merged_vector = Concatenate(axis=-1)(layer)\n",
    "\n",
    "    #camadas internas\n",
    "    layer = merged_vector\n",
    "    for i in range(len(param_v_a_2)):\n",
    "        layer = Dense(tam*(tam+1)*param_v_a_2[i],activation='relu', kernel_regularizer=l2(1e-4))(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = Dropout(0.1)(layer)\n",
    "\n",
    "    #camada de saida\n",
    "    output_layer = Dense(1)(layer)\n",
    "\n",
    "    #compilar modelo\n",
    "    model = Model(inputs=inputArray, outputs=output_layer)\n",
    "    adam = optimizers.Adam(learning_rate=param_v_l, clipnorm=1.0)\n",
    "    model.compile(optimizer=adam, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Criar os datasets de treino\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator,  # Gera um novo generator a cada época\n",
    "    args=(param_v_b, True, True),\n",
    "    output_signature=output_signature_type_bound(param_v_b)\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#dataset de validação\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator,\n",
    "    args=(param_v_b, True, False),\n",
    "    output_signature=output_signature_type_bound(param_v_b)\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "now = datetime.now()\n",
    "# Treinar modelo\n",
    "print(\"treinamento rede bound iniciado\")\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=1000,\n",
    "    initial_epoch=initial_epoch,\n",
    "    verbose=1,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[\n",
    "        printbatch(),\n",
    "        EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True),\n",
    "        ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=False),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
    "    ]\n",
    ")\n",
    "final_model_value_path = os.path.join(output_path, f\"dnn_value_model_{tam}.keras\")\n",
    "model.save(final_model_value_path)\n",
    "del model, inputArray, merged_vector, layer, output_layer, train_dataset, val_dataset\n",
    "print(\"rede 2 treinada\")\n",
    "clear_session()\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "cuda.select_device(0)\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
